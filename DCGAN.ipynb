{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGAN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPy6r2PyApTo/+X6v3w5NtG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jvallalta/primeros-pasos/blob/main/DCGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FubEMYzjZkVV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sHPf26oZnwz"
      },
      "source": [
        "# DCGAN: Primeros Pasos\n",
        "## Activaciones\n",
        "\n",
        "Las activaciones son funciones que tienen como entrada un número real y generan a la salida un número real en un cierto rango, utilizando una funcion diferenciable no lineal. La diferencialidad es necesaria para backpropagation y la no linealidad para modelar funciones complejas.\n",
        "\n",
        "Las funciones de activación más comunes son:\n",
        "\n",
        "- ReLU\n",
        "- Leaky ReLU\n",
        "- Sigmoid\n",
        "- Tanh\n",
        "\n",
        "ReLU es una de las funciones más utilizadas: ReLU(X) = max(0,x). Tiene como problema el Dying ReLU problem por el que la red puede dejar de aprender que se resuelve con la funcion Leaky ReLU (usa generalmente un valor de 0.1).\n",
        "\n",
        "La Sigmoide genera valores en 0 y 1. Se utiliza generalmente en los modelos de clasificación binario en la última capa para generar la probabildad de una clase. No se suele utilzar en  capas intermedias por los efectos del gradiente desvaneciente y los problemas de saturación.\n",
        "\n",
        "Una función similar es la tangente hiperbólica con valores en [-1,1].  Una diferencia con la sigmoide es que mantiene el signo de la entrada. Presenta los mismos problemas que la sigmoide para las capas intermedias.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-C8WTYVci9P"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}